{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04b4a513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.utils as np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88d7b053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eb9e5a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">review</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>25000</td>\n",
       "      <td>24698</td>\n",
       "      <td>This show comes up with interesting locations ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>25000</td>\n",
       "      <td>24884</td>\n",
       "      <td>Loved today's show!!! It was a variety and not...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          review                                                            \\\n",
       "           count unique                                                top   \n",
       "sentiment                                                                    \n",
       "negative   25000  24698  This show comes up with interesting locations ...   \n",
       "positive   25000  24884  Loved today's show!!! It was a variety and not...   \n",
       "\n",
       "                \n",
       "          freq  \n",
       "sentiment       \n",
       "negative     3  \n",
       "positive     5  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('sentiment').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "525f1002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  One of the other reviewers has mentioned that ...          1\n",
       "1  A wonderful little production. <br /><br />The...          1\n",
       "2  I thought this was a wonderful way to spend ti...          1\n",
       "3  Basically there's a family where a little boy ...          0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...          1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment']=df['sentiment'].apply(lambda x: 1 if x=='positive' else 0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f0057a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">review</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25000</td>\n",
       "      <td>24698</td>\n",
       "      <td>This show comes up with interesting locations ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25000</td>\n",
       "      <td>24884</td>\n",
       "      <td>Loved today's show!!! It was a variety and not...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          review                                                            \\\n",
       "           count unique                                                top   \n",
       "sentiment                                                                    \n",
       "0          25000  24698  This show comes up with interesting locations ...   \n",
       "1          25000  24884  Loved today's show!!! It was a variety and not...   \n",
       "\n",
       "                \n",
       "          freq  \n",
       "sentiment       \n",
       "0            3  \n",
       "1            5  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('sentiment').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a07442b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13704\n",
      "32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengthOFReviews = [len(a) for a in df.review.values.tolist()]\n",
    "print(max(lengthOFReviews)), print(min(lengthOFReviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1c1f3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = list(df['review']),list(df['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1801464b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3563733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, stratify=df['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f9d518a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y_test = encoder.transform(y_test)\n",
    "encoded_Y_train = encoder.transform(y_train)\n",
    "\n",
    "# YY = tf.keras.utils.to_categorical(\n",
    "#     y_train, num_classes=2, dtype='float32'\n",
    "# )\n",
    "\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y_test = np_utils.to_categorical(encoded_Y_test)\n",
    "dummy_y_train = np_utils.to_categorical(encoded_Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab4fe3e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37500,), (12500,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_Y_train.shape , encoded_Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "233b4de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12500, 2), (37500, 2))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_y_test.shape , dummy_y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e2c97685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertTokenizer\n",
    "# from transformers import TFBertForSequenceClassification\n",
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "# bert_preprocess = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
    "# bert_encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e5f8bf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5bebb437",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using C:\\Users\\Phoenix\\AppData\\Local\\Temp\\tfhub_modules to cache modules.\n",
      "INFO:absl:Downloading TF-Hub Module 'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2'.\n",
      "INFO:absl:Downloading https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2: 24.49MB\n",
      "INFO:absl:Downloading https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2: 44.49MB\n",
      "INFO:absl:Downloading https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2: 64.49MB\n",
      "INFO:absl:Downloading https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2: 84.49MB\n",
      "INFO:absl:Downloading https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2: 104.49MB\n",
      "INFO:absl:Downloading https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2: 124.49MB\n",
      "INFO:absl:Downloading https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2: 144.49MB\n",
      "INFO:absl:Downloading https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2: 164.49MB\n",
      "INFO:absl:Downloading https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2: 184.49MB\n",
      "INFO:absl:Downloading https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2: 204.49MB\n",
      "INFO:absl:Downloading https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2: 224.49MB\n",
      "INFO:absl:Downloading https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2: 244.49MB\n",
      "INFO:absl:Downloading https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2: 264.49MB\n",
      "INFO:absl:Downloading https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2: 284.49MB\n",
      "INFO:absl:Downloading https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2: 304.49MB\n",
      "INFO:absl:Downloading https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2: 324.49MB\n",
      "INFO:absl:Downloading https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2: 344.49MB\n",
      "INFO:absl:Downloading https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2: 364.49MB\n",
      "INFO:absl:Downloading https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2: 384.49MB\n",
      "INFO:absl:Downloading https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2: 404.49MB\n",
      "INFO:absl:Downloading https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2: 424.49MB\n",
      "INFO:absl:Downloading https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2: 444.49MB\n",
      "INFO:absl:Downloading https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2: 464.49MB\n",
      "INFO:absl:Downloading https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2: 484.49MB\n",
      "INFO:absl:Downloading https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2: 504.49MB\n",
      "INFO:absl:Downloading https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2: 524.49MB\n",
      "INFO:absl:Downloading https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2: 544.49MB\n",
      "INFO:absl:Downloading https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2: 564.49MB\n",
      "INFO:absl:Downloading https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2: 584.49MB\n",
      "INFO:absl:Downloading https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2: 604.49MB\n",
      "INFO:absl:Downloading https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2: 624.49MB\n",
      "INFO:absl:Downloading https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2: 644.49MB\n",
      "INFO:absl:Downloading https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2: 664.49MB\n",
      "INFO:absl:Downloaded https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2, Total size: 683.05MB\n",
      "INFO:absl:Downloaded TF-Hub Module 'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2'.\n"
     ]
    }
   ],
   "source": [
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2\",\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "49c49219",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-models-official\n",
      "  Using cached tf_models_official-2.6.0-py2.py3-none-any.whl (1.8 MB)\n",
      "Requirement already satisfied: tensorflow-text>=2.5.0 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from tf-models-official) (2.6.0)\n",
      "Requirement already satisfied: tensorflow-model-optimization>=0.4.1 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from tf-models-official) (0.7.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from tf-models-official) (5.4.1)\n",
      "Collecting psutil>=5.4.3\n",
      "  Using cached psutil-5.8.0-cp39-cp39-win_amd64.whl (246 kB)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from tf-models-official) (1.7.1)\n",
      "Requirement already satisfied: tensorflow>=2.5.0 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from tf-models-official) (2.6.0)\n",
      "Collecting oauth2client\n",
      "  Using cached oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
      "Requirement already satisfied: Cython in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from tf-models-official) (0.29.24)\n",
      "Requirement already satisfied: sacrebleu in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from tf-models-official) (2.0.0)\n",
      "Collecting google-api-python-client>=1.6.7\n",
      "  Using cached google_api_python_client-2.26.1-py2.py3-none-any.whl (7.6 MB)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from tf-models-official) (3.4.3)\n",
      "Requirement already satisfied: tf-slim>=1.1.0 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from tf-models-official) (1.1.0)\n",
      "Collecting pycocotools\n",
      "  Using cached pycocotools-2.0.2.tar.gz (23 kB)\n",
      "Collecting gin-config\n",
      "  Using cached gin_config-0.4.0-py2.py3-none-any.whl (46 kB)\n",
      "Requirement already satisfied: Pillow in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from tf-models-official) (8.3.2)\n",
      "Collecting py-cpuinfo>=3.3.0\n",
      "  Using cached py_cpuinfo-8.0.0-py3-none-any.whl\n",
      "Requirement already satisfied: pandas>=0.22.0 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from tf-models-official) (1.3.3)\n",
      "Requirement already satisfied: seqeval in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from tf-models-official) (1.2.2)\n",
      "Collecting kaggle>=1.3.9\n",
      "  Using cached kaggle-1.5.12-py3-none-any.whl\n",
      "Requirement already satisfied: tensorflow-addons in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from tf-models-official) (0.14.0)\n",
      "Requirement already satisfied: six in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from tf-models-official) (1.15.0)\n",
      "Requirement already satisfied: tensorflow-hub>=0.6.0 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from tf-models-official) (0.12.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from tf-models-official) (0.1.96)\n",
      "Requirement already satisfied: tensorflow-datasets in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from tf-models-official) (4.4.0)\n",
      "Requirement already satisfied: numpy>=1.15.4 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from tf-models-official) (1.19.5)\n",
      "Collecting opencv-python-headless\n",
      "  Using cached opencv_python_headless-4.5.3.56-cp39-cp39-win_amd64.whl (34.8 MB)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=1.16.0 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from google-api-python-client>=1.6.7->tf-models-official) (1.35.0)\n",
      "Requirement already satisfied: google-api-core<3.0.0dev,>=1.21.0 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from google-api-python-client>=1.6.7->tf-models-official) (2.1.1)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from google-api-python-client>=1.6.7->tf-models-official) (0.20.1)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.0 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from google-api-python-client>=1.6.7->tf-models-official) (4.1.1)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.1.0 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from google-api-python-client>=1.6.7->tf-models-official) (0.1.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from google-api-core<3.0.0dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (58.0.4)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from google-api-core<3.0.0dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (1.53.0)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from google-api-core<3.0.0dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (3.18.0)\n",
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from google-api-core<3.0.0dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (2.26.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'C:\\Users\\Phoenix\\anaconda3\\envs\\my_learning_env\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\Phoenix\\\\AppData\\\\Local\\\\Temp\\\\pip-install-s3e4c4jx\\\\pycocotools_5e6a49664d7e406fb55fd09b44c9425e\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\Phoenix\\\\AppData\\\\Local\\\\Temp\\\\pip-install-s3e4c4jx\\\\pycocotools_5e6a49664d7e406fb55fd09b44c9425e\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\Phoenix\\AppData\\Local\\Temp\\pip-wheel-srt40wx4'\n",
      "       cwd: C:\\Users\\Phoenix\\AppData\\Local\\Temp\\pip-install-s3e4c4jx\\pycocotools_5e6a49664d7e406fb55fd09b44c9425e\\\n",
      "  Complete output (16 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-3.9\n",
      "  creating build\\lib.win-amd64-3.9\\pycocotools\n",
      "  copying pycocotools\\coco.py -> build\\lib.win-amd64-3.9\\pycocotools\n",
      "  copying pycocotools\\cocoeval.py -> build\\lib.win-amd64-3.9\\pycocotools\n",
      "  copying pycocotools\\mask.py -> build\\lib.win-amd64-3.9\\pycocotools\n",
      "  copying pycocotools\\__init__.py -> build\\lib.win-amd64-3.9\\pycocotools\n",
      "  running build_ext\n",
      "  cythoning pycocotools/_mask.pyx to pycocotools\\_mask.c\n",
      "  C:\\Users\\Phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\Phoenix\\AppData\\Local\\Temp\\pip-install-s3e4c4jx\\pycocotools_5e6a49664d7e406fb55fd09b44c9425e\\pycocotools\\_mask.pyx\n",
      "    tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "  building 'pycocotools._mask' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for pycocotools\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\Phoenix\\anaconda3\\envs\\my_learning_env\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\Phoenix\\\\AppData\\\\Local\\\\Temp\\\\pip-install-s3e4c4jx\\\\pycocotools_5e6a49664d7e406fb55fd09b44c9425e\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\Phoenix\\\\AppData\\\\Local\\\\Temp\\\\pip-install-s3e4c4jx\\\\pycocotools_5e6a49664d7e406fb55fd09b44c9425e\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\Phoenix\\AppData\\Local\\Temp\\pip-record-fj48k_pb\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\Users\\Phoenix\\anaconda3\\envs\\my_learning_env\\Include\\pycocotools'\n",
      "         cwd: C:\\Users\\Phoenix\\AppData\\Local\\Temp\\pip-install-s3e4c4jx\\pycocotools_5e6a49664d7e406fb55fd09b44c9425e\\\n",
      "    Complete output (14 lines):\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build\\lib.win-amd64-3.9\n",
      "    creating build\\lib.win-amd64-3.9\\pycocotools\n",
      "    copying pycocotools\\coco.py -> build\\lib.win-amd64-3.9\\pycocotools\n",
      "    copying pycocotools\\cocoeval.py -> build\\lib.win-amd64-3.9\\pycocotools\n",
      "    copying pycocotools\\mask.py -> build\\lib.win-amd64-3.9\\pycocotools\n",
      "    copying pycocotools\\__init__.py -> build\\lib.win-amd64-3.9\\pycocotools\n",
      "    running build_ext\n",
      "    skipping 'pycocotools\\_mask.c' Cython extension (up-to-date)\n",
      "    building 'pycocotools._mask' extension\n",
      "    error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "    ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: 'C:\\Users\\Phoenix\\anaconda3\\envs\\my_learning_env\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\Phoenix\\\\AppData\\\\Local\\\\Temp\\\\pip-install-s3e4c4jx\\\\pycocotools_5e6a49664d7e406fb55fd09b44c9425e\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\Phoenix\\\\AppData\\\\Local\\\\Temp\\\\pip-install-s3e4c4jx\\\\pycocotools_5e6a49664d7e406fb55fd09b44c9425e\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\Phoenix\\AppData\\Local\\Temp\\pip-record-fj48k_pb\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\Users\\Phoenix\\anaconda3\\envs\\my_learning_env\\Include\\pycocotools' Check the logs for full command output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from google-auth<3.0.0dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from google-auth<3.0.0dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from google-auth<3.0.0dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official) (0.2.8)\n",
      "Requirement already satisfied: pyparsing<3,>=2.4.2 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from httplib2<1dev,>=0.15.0->google-api-python-client>=1.6.7->tf-models-official) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from kaggle>=1.3.9->tf-models-official) (2.8.2)\n",
      "Requirement already satisfied: certifi in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from kaggle>=1.3.9->tf-models-official) (2021.5.30)\n",
      "Requirement already satisfied: python-slugify in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from kaggle>=1.3.9->tf-models-official) (5.0.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from kaggle>=1.3.9->tf-models-official) (4.62.3)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from kaggle>=1.3.9->tf-models-official) (1.26.7)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from pandas>=0.22.0->tf-models-official) (2021.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official) (0.4.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3.0.0dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3.0.0dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (2.0.6)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from tensorflow>=2.5.0->tf-models-official) (1.1.2)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from tensorflow>=2.5.0->tf-models-official) (1.12)\n",
      "Requirement already satisfied: clang~=5.0 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from tensorflow>=2.5.0->tf-models-official) (5.0)\n",
      "Requirement already satisfied: keras~=2.6 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from tensorflow>=2.5.0->tf-models-official) (2.6.0)\n",
      "Requirement already satisfied: h5py~=3.1.0 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from tensorflow>=2.5.0->tf-models-official) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from tensorflow>=2.5.0->tf-models-official) (3.7.4.3)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from tensorflow>=2.5.0->tf-models-official) (1.1.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from tensorflow>=2.5.0->tf-models-official) (0.12.0)\n",
      "Requirement already satisfied: gast==0.4.0 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from tensorflow>=2.5.0->tf-models-official) (0.4.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.37.0 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from tensorflow>=2.5.0->tf-models-official) (1.41.0)\n",
      "Requirement already satisfied: tensorboard~=2.6 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from tensorflow>=2.5.0->tf-models-official) (2.6.0)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from tensorflow>=2.5.0->tf-models-official) (0.37.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from tensorflow>=2.5.0->tf-models-official) (1.12.1)\n",
      "Requirement already satisfied: tensorflow-estimator~=2.6 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from tensorflow>=2.5.0->tf-models-official) (2.6.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from tensorflow>=2.5.0->tf-models-official) (3.3.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from tensorflow>=2.5.0->tf-models-official) (1.6.3)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from tensorflow>=2.5.0->tf-models-official) (0.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from tensorboard~=2.6->tensorflow>=2.5.0->tf-models-official) (3.3.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from tensorboard~=2.6->tensorflow>=2.5.0->tf-models-official) (2.0.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from tensorboard~=2.6->tensorflow>=2.5.0->tf-models-official) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from tensorboard~=2.6->tensorflow>=2.5.0->tf-models-official) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from tensorboard~=2.6->tensorflow>=2.5.0->tf-models-official) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.5.0->tf-models-official) (1.3.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.5.0->tf-models-official) (3.1.1)\n",
      "Requirement already satisfied: dm-tree~=0.1.1 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official) (0.1.6)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from matplotlib->tf-models-official) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from matplotlib->tf-models-official) (1.3.2)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from python-slugify->kaggle>=1.3.9->tf-models-official) (1.3)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from sacrebleu->tf-models-official) (0.8.9)\n",
      "Requirement already satisfied: regex in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from sacrebleu->tf-models-official) (2021.9.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from sacrebleu->tf-models-official) (0.4.4)\n",
      "Requirement already satisfied: portalocker in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from sacrebleu->tf-models-official) (2.3.2)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from portalocker->sacrebleu->tf-models-official) (228)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from seqeval->tf-models-official) (1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official) (1.0.1)\n",
      "Requirement already satisfied: typeguard>=2.7 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from tensorflow-addons->tf-models-official) (2.13.0)\n",
      "Requirement already satisfied: dill in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from tensorflow-datasets->tf-models-official) (0.3.4)\n",
      "Requirement already satisfied: future in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from tensorflow-datasets->tf-models-official) (0.18.2)\n",
      "Requirement already satisfied: attrs>=18.1.0 in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from tensorflow-datasets->tf-models-official) (21.2.0)\n",
      "Requirement already satisfied: tensorflow-metadata in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from tensorflow-datasets->tf-models-official) (1.2.0)\n",
      "Requirement already satisfied: promise in c:\\users\\phoenix\\anaconda3\\envs\\my_learning_env\\lib\\site-packages (from tensorflow-datasets->tf-models-official) (2.3)\n",
      "Building wheels for collected packages: pycocotools\n",
      "  Building wheel for pycocotools (setup.py): started\n",
      "  Building wheel for pycocotools (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for pycocotools\n",
      "Failed to build pycocotools\n",
      "Installing collected packages: pycocotools, py-cpuinfo, psutil, opencv-python-headless, oauth2client, kaggle, google-api-python-client, gin-config, tf-models-official\n",
      "    Running setup.py install for pycocotools: started\n",
      "    Running setup.py install for pycocotools: finished with status 'error'\n"
     ]
    }
   ],
   "source": [
    "pip install -U tf-models-official"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bde8660e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'official'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16268/35414779.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mofficial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbert_models\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mofficial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mofficial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_classifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mofficial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenization\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtokenization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'official'"
     ]
    }
   ],
   "source": [
    "import official.nlp.bert.bert_models\n",
    "import official.nlp.bert.configs\n",
    "import official.nlp.bert.run_classifier\n",
    "import official.nlp.bert.tokenization as tokenization\n",
    "\n",
    "from official.modeling import tf_utils\n",
    "from official import nlp\n",
    "from official.nlp import bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2bd0cd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "# tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "773a119b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (660 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "def encode_names(n):\n",
    "    tokens = list(tokenizer.tokenize(n))\n",
    "    tokens.append('[SEP]')  # seperation token. Would bemuch more useful if you had a multiple text input.\n",
    "    return tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "texts = tf.ragged.constant([encode_names(n) for n in X_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fedae652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It\\'s amazing that actress P.J. Soles didn\\'t become a big star after playing Riff Randall, #1 fan of the punk rock group the Ramones, in \"Rock \\'n\\' Roll High School\". Soles is so exuberant, you don\\'t mind she\\'s obviously too old to still be in high school (that fact is leveled out by having all the kids look 24). The movie is a fast-paced frolic that doesn\\'t cop-out; everything gets blown to smithereens at the end, and that\\'s just as it should be. Mary Woronov, an innately kinky and funny presence as the Nazi-like principal, gets a great, one-of-a-kind bit at the beginning where Frisbees fly dangerously close to her head (how many takes did they use on that, or was it a fluke?) and Dey Young is very appealing as Soles\\' best, Kate Rambeau. The weakest link, ironically enough, in this \"High School\" chain-gang is the Ramones. They can\\'t act, they\\'re not funny, and their concert segment goes on too long. One Ramones song, \"I Want You Around\", is treated as a fantasy and is well captured; other incidental songs are good, particularly a rare Paul McCartney ballad heard near the beginning (\"Did We Meet Somewhere Before?\"). Great fun! *** from ****'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fadc5075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(311,), dtype=int32, numpy=\n",
       "array([ 2009,  1005,  1055,  6429,  2008,  3883,  1052,  1012,  1046,\n",
       "        1012,  7082,  2015,  2134,  1005,  1056,  2468,  1037,  2502,\n",
       "        2732,  2044,  2652, 24808, 12813,  1010,  1001,  1015,  5470,\n",
       "        1997,  1996,  7196,  2600,  2177,  1996, 12716,  2229,  1010,\n",
       "        1999,  1000,  2600,  1005,  1050,  1005,  4897,  2152,  2082,\n",
       "        1000,  1012,  7082,  2015,  2003,  2061,  4654, 21436,  4630,\n",
       "        1010,  2017,  2123,  1005,  1056,  2568,  2016,  1005,  1055,\n",
       "        5525,  2205,  2214,  2000,  2145,  2022,  1999,  2152,  2082,\n",
       "        1006,  2008,  2755,  2003, 22915,  2041,  2011,  2383,  2035,\n",
       "        1996,  4268,  2298,  2484,  1007,  1012,  1996,  3185,  2003,\n",
       "        1037,  3435,  1011, 13823, 10424, 23518,  2008,  2987,  1005,\n",
       "        1056,  8872,  1011,  2041,  1025,  2673,  4152, 10676,  2000,\n",
       "        3044,  7869,  6132,  2012,  1996,  2203,  1010,  1998,  2008,\n",
       "        1005,  1055,  2074,  2004,  2009,  2323,  2022,  1012,  2984,\n",
       "       24185,  4948,  4492,  1010,  2019, 25605,  2135, 12631,  4801,\n",
       "        1998,  6057,  3739,  2004,  1996,  6394,  1011,  2066,  4054,\n",
       "        1010,  4152,  1037,  2307,  1010,  2028,  1011,  1997,  1011,\n",
       "        1037,  1011,  2785,  2978,  2012,  1996,  2927,  2073, 10424,\n",
       "        2483, 11306,  2015,  4875, 20754,  2485,  2000,  2014,  2132,\n",
       "        1006,  2129,  2116,  3138,  2106,  2027,  2224,  2006,  2008,\n",
       "        1010,  2030,  2001,  2009,  1037, 19857,  3489,  1029,  1007,\n",
       "        1998,  2139,  2100,  2402,  2003,  2200, 16004,  2004,  7082,\n",
       "        2015,  1005,  2190,  1010,  5736,  8223, 26401,  1012,  1996,\n",
       "        5410,  4355,  4957,  1010, 18527,  2438,  1010,  1999,  2023,\n",
       "        1000,  2152,  2082,  1000,  4677,  1011,  6080,  2003,  1996,\n",
       "       12716,  2229,  1012,  2027,  2064,  1005,  1056,  2552,  1010,\n",
       "        2027,  1005,  2128,  2025,  6057,  1010,  1998,  2037,  4164,\n",
       "        6903,  3632,  2006,  2205,  2146,  1012,  2028, 12716,  2229,\n",
       "        2299,  1010,  1000,  1045,  2215,  2017,  2105,  1000,  1010,\n",
       "        2003,  5845,  2004,  1037,  5913,  1998,  2003,  2092,  4110,\n",
       "        1025,  2060,  5043,  2389,  2774,  2024,  2204,  1010,  3391,\n",
       "        1037,  4678,  2703, 15320, 11571,  2657,  2379,  1996,  2927,\n",
       "        1006,  1000,  2106,  2057,  3113,  4873,  2077,  1029,  1000,\n",
       "        1007,  1012,  2307,  4569,   999,  1008,  1008,  1008,  2013,\n",
       "        1008,  1008,  1008,  1008,   102])>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "efd02a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*texts.shape[0]\n",
    "input_word_ids = tf.concat([cls, texts], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e33c595c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_mask = tf.ones_like(input_word_ids).to_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "510bc6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_cls = tf.zeros_like(cls)\n",
    "type_texts = tf.ones_like(texts)\n",
    "input_type_ids = tf.concat([type_cls, type_texts], axis=-1).to_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f4ff6d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length is: 3157\n"
     ]
    }
   ],
   "source": [
    "lens = [len(i) for i in input_word_ids]\n",
    "\n",
    "max_seq_length = max(lens)\n",
    "print('Max length is:', max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e95a78a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_names(n, tokenizer):\n",
    "    tokens = list(tokenizer.tokenize(n))\n",
    "    tokens.append('[SEP]')\n",
    "    return tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "def bert_encode(string_list, tokenizer, max_seq_length):\n",
    "    num_examples = len(string_list)\n",
    "\n",
    "    string_tokens = tf.ragged.constant([\n",
    "      encode_names(n, tokenizer) for n in np.array(string_list)])\n",
    "\n",
    "    cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*string_tokens.shape[0]\n",
    "    input_word_ids = tf.concat([cls, string_tokens], axis=-1)\n",
    "\n",
    "    input_mask = tf.ones_like(input_word_ids).to_tensor(shape=(None, max_seq_length))\n",
    "\n",
    "    type_cls = tf.zeros_like(cls)\n",
    "    type_tokens = tf.ones_like(string_tokens)\n",
    "    input_type_ids = tf.concat(\n",
    "      [type_cls, type_tokens], axis=-1).to_tensor(shape=(None, max_seq_length))\n",
    "\n",
    "    inputs = {\n",
    "      'input_word_ids': input_word_ids.to_tensor(shape=(None, max_seq_length)),\n",
    "      'input_mask': input_mask,\n",
    "      'input_type_ids': input_type_ids}\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5064e61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = bert_encode(X_train, tokenizer, max_seq_length)\n",
    "x_test = bert_encode(X_test, tokenizer, max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6ce4198f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = len(encoder.classes_)  # Based on available class selection\n",
    "max_seq_length = max_seq_length  # we calculated this a couple cells ago\n",
    "\n",
    "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                       name=\"input_word_ids\")\n",
    "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                   name=\"input_mask\")\n",
    "segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                    name=\"segment_ids\")\n",
    "\n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])                                  \n",
    "\n",
    "output = tf.keras.layers.Dropout(rate=0.1)(pooled_output)\n",
    "\n",
    "output = tf.keras.layers.Dense(num_class, activation='softmax', name='output')(output)\n",
    "\n",
    "model = tf.keras.Model(\n",
    "    inputs={\n",
    "        'input_word_ids': input_word_ids,\n",
    "        'input_mask': input_mask,\n",
    "        'input_type_ids': segment_ids\n",
    "        },\n",
    "        outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6abf1a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "batch_size = 16  # select based on your GPU resources\n",
    "eval_batch_size = batch_size\n",
    "\n",
    "train_data_size = len(dummy_y_train)\n",
    "steps_per_epoch = int(train_data_size / batch_size)\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "warmup_steps = int(epochs * train_data_size * 0.1 / batch_size)\n",
    "\n",
    "# optimizer = nlp.optimization.create_optimizer(\n",
    "#     2e-5, num_train_steps=num_train_steps, num_warmup_steps=warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0f691876",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "30f960ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 3157)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 3157)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 3157)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 768), (None, 177853441   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 768)          0           keras_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            1538        dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 177,854,979\n",
      "Trainable params: 177,854,978\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b0fbc4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " Incompatible shapes: [16,3157,768] vs. [512,768]\n\t [[{{node Adam/gradients/StatefulPartitionedCall/gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1019/gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1019/gradients/transformer_encoder/StatefulPartitionedCall_grad/StatefulPartitionedCall/gradients/position_embedding/BroadcastTo_grad/BroadcastGradientArgs}}]] [Op:__inference_train_function_902384]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16268/4262693935.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m history = model.fit(x_train,\n\u001b[0m\u001b[0;32m      2\u001b[0m                     \u001b[0mdummy_y_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdummy_y_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\my_learning_env\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\my_learning_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\my_learning_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    948\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\my_learning_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3037\u001b[0m       (graph_function,\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\my_learning_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1963\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\my_learning_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\my_learning_env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m:  Incompatible shapes: [16,3157,768] vs. [512,768]\n\t [[{{node Adam/gradients/StatefulPartitionedCall/gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1019/gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1019/gradients/transformer_encoder/StatefulPartitionedCall_grad/StatefulPartitionedCall/gradients/position_embedding/BroadcastTo_grad/BroadcastGradientArgs}}]] [Op:__inference_train_function_902384]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train,\n",
    "                    dummy_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(X_test, dummy_y_test),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29380b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
